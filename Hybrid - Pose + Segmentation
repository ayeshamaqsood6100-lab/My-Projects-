# @title üöÄ HYBRID: RTMLib Pose + Sapiens Segmentation (Best of Both)
# ============================================================================
# Uses RTMLib Wholebody for ACCURATE pose detection (133 keypoints)
# Uses Sapiens 1B for body segmentation overlay
# Combines both for the best possible output
# ============================================================================

import os
import sys
import subprocess
import cv2
import numpy as np
import gc
from tqdm import tqdm

# ============================================================================
# PHASE 0: INSTALL DEPENDENCIES
# ============================================================================
print("üîß PHASE 0: INSTALLING DEPENDENCIES")

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

install("torch")
install("torchvision")
install("rtmlib")
install("onnxruntime-gpu")
install("opencv-python")
install("tqdm")
install("huggingface_hub")
print("‚úÖ Dependencies installed!\n")

import torch
from huggingface_hub import hf_hub_download
from rtmlib import Wholebody, draw_skeleton

# ============================================================================
# PHASE 1: SETUP
# ============================================================================
print("üöÄ PHASE 1: SETUP")
try:
    from google.colab import drive
    import time
    IN_COLAB = True
    for attempt in range(3):
        try:
            if not os.path.exists('/content/drive'):
                drive.mount('/content/drive')
            print("‚úÖ Google Drive mounted!")
            break
        except ValueError as e:
            if attempt < 2:
                print(f"‚ö†Ô∏è Drive mount failed (attempt {attempt+1}/3), retrying...")
                time.sleep(5)
            else:
                IN_COLAB = False
except ImportError:
    IN_COLAB = False
    print("‚ö†Ô∏è Not running in Colab.")

MODEL_DIR = "/content/sapiens_models" if IN_COLAB else "./sapiens_models"

print("\nüì¶ Downloading Sapiens Segmentation Model...")
SEG_MODEL_PATH = hf_hub_download(
    repo_id="facebook/sapiens-seg-1b-torchscript",
    filename="sapiens_1b_goliath_best_goliath_mIoU_7994_epoch_151_torchscript.pt2",
    local_dir=MODEL_DIR
)
print("‚úÖ Sapiens Segmentation Model Ready!")

# ============================================================================
# CONFIGURATION
# ============================================================================
VIDEO_FILENAME = "Brother.mp4"
BASE_PATH = "/content/drive/MyDrive/ConnectHearTestVideos" if IN_COLAB else "."
RAW_INPUT = os.path.join(BASE_PATH, VIDEO_FILENAME)
CLEAN_INPUT = "/content/cleaned_input.mp4" if IN_COLAB else "./cleaned_input.mp4"
OUTPUT_POSE = os.path.join(BASE_PATH, f"HYBRID_pose_{VIDEO_FILENAME}")
OUTPUT_SEG = os.path.join(BASE_PATH, f"HYBRID_seg_{VIDEO_FILENAME}")
OUTPUT_COMBINED = os.path.join(BASE_PATH, f"HYBRID_combined_{VIDEO_FILENAME}")

# RTMLib settings
POSE_THRESHOLD = 0.5  # Same as your working RTM code
LINE_WIDTH = 2
RADIUS = 2

# Check input
print(f"\nüîç Checking Input: {RAW_INPUT}")
if not os.path.exists(RAW_INPUT):
    print(f"‚ùå ERROR: File not found at {RAW_INPUT}")
    sys.exit(1)

print("üîß Repairing video...")
subprocess.run(f'ffmpeg -y -loglevel error -i "{RAW_INPUT}" -c:v libx264 -preset fast -crf 22 -pix_fmt yuv420p "{CLEAN_INPUT}"', shell=True)

# ============================================================================
# SEGMENTATION COLORS
# ============================================================================
BODY_PART_COLORS = np.array([
    [0,0,0], [255,220,200], [255,200,170], [100,200,100], [50,255,50],
    [255,165,0], [0,255,128], [255,140,0], [0,255,255], [255,100,100],
    [100,100,255], [255,100,255], [50,50,200], [200,50,200], [0,200,200],
    [200,200,0], [139,90,43], [255,210,180], [255,255,255], [255,100,150]
], dtype=np.uint8)

# ============================================================================
# PREPROCESSING
# ============================================================================
def preprocess_seg(img):
    """Preprocess for Sapiens segmentation."""
    img = cv2.resize(img, (768, 1024))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
    img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]
    return torch.from_numpy(img.transpose(2, 0, 1)).unsqueeze(0).float()

def draw_segmentation(seg_map, w, h, frame, alpha=0.5):
    """Draw segmentation overlay."""
    seg_colored = BODY_PART_COLORS[seg_map % len(BODY_PART_COLORS)]
    seg_colored = cv2.resize(seg_colored, (w, h), interpolation=cv2.INTER_LINEAR)
    mask = (seg_map != 0).astype(np.uint8)
    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_LINEAR)
    mask = np.expand_dims(mask, axis=2)
    blended = (seg_colored * alpha + frame * (1 - alpha)).astype(np.uint8)
    return np.where(mask > 0, blended, frame).astype(np.uint8)

# ============================================================================
# MAIN PROCESSING
# ============================================================================
print("\nüé¨ INITIALIZING...")

cap = cv2.VideoCapture(CLEAN_INPUT)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
fps = cap.get(cv2.CAP_PROP_FPS) or 30
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
width, height = (width // 2) * 2, (height // 2) * 2
cap.release()

print(f"   Video: {width}x{height}, {total_frames} frames, {fps} fps")
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"   Device: {device}")

# Initialize RTMLib Wholebody (same as your working code)
print("\nüîÑ Loading RTMLib Wholebody model...")
pose_model = Wholebody(to_openpose=False, mode='performance', backend='onnxruntime', device=device)
print("‚úÖ RTMLib Wholebody Ready!")

# Initialize Sapiens Segmentation
print("üîÑ Loading Sapiens Segmentation model...")
seg_model = torch.jit.load(SEG_MODEL_PATH, map_location=device).eval()
print("‚úÖ Sapiens Segmentation Ready!")

# Setup outputs
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out_pose = cv2.VideoWriter("/content/temp_pose.mp4", fourcc, fps, (width, height))
out_seg = cv2.VideoWriter("/content/temp_seg.mp4", fourcc, fps, (width, height))
out_combined = cv2.VideoWriter("/content/temp_combined.mp4", fourcc, fps, (width * 2, height))

cap = cv2.VideoCapture(CLEAN_INPUT)
print("\nüèÉ Processing (RTMLib Pose + Sapiens Seg)...")

with tqdm(total=total_frames, desc="Processing") as pbar:
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.resize(frame, (width, height))

        # === RTMLib Pose Detection (ACCURATE!) ===
        keypoints, scores = pose_model(frame)

        # Single person filter (from your working code)
        if len(keypoints) > 0:
            avg_scores = [np.mean(s) for s in scores]
            best_idx = np.argmax(avg_scores)
            keypoints = keypoints[best_idx:best_idx+1]
            scores = scores[best_idx:best_idx+1]

            # REMOVE LOWER BODY: Knees (13,14) and Ankles (15,16) - matches RTM notebook
            scores[:, 13:17] = 0.0

        # Draw pose using RTMLib's draw_skeleton
        pose_frame = draw_skeleton(frame.copy(), keypoints, scores,
                                   kpt_thr=POSE_THRESHOLD,
                                   line_width=LINE_WIDTH,
                                   radius=RADIUS)

        # === Sapiens Segmentation ===
        tensor = preprocess_seg(frame).to(device)
        with torch.no_grad():
            seg_out = seg_model(tensor)
        seg_logits = (seg_out[0] if isinstance(seg_out, tuple) else seg_out)[0].cpu().numpy()
        seg_map = np.argmax(seg_logits, axis=0)

        seg_frame = draw_segmentation(seg_map, width, height, frame.copy())

        # === Combine: Pose on left, Segmentation on right ===
        combined_frame = np.hstack([pose_frame, seg_frame])

        out_pose.write(pose_frame)
        out_seg.write(seg_frame)
        out_combined.write(combined_frame)

        pbar.update(1)

cap.release()
out_pose.release()
out_seg.release()
out_combined.release()

del pose_model, seg_model
gc.collect()
if device == 'cuda':
    torch.cuda.empty_cache()

# Convert to proper MP4 and save
print("\nüîÑ Finalizing...")

def convert_and_save(temp, final):
    subprocess.run(f'ffmpeg -y -loglevel error -i "{temp}" -c:v libx264 -pix_fmt yuv420p "{final}"', shell=True)
    if os.path.exists(temp):
        os.remove(temp)

convert_and_save("/content/temp_pose.mp4", OUTPUT_POSE)
convert_and_save("/content/temp_seg.mp4", OUTPUT_SEG)
convert_and_save("/content/temp_combined.mp4", OUTPUT_COMBINED)

if os.path.exists(CLEAN_INPUT):
    os.remove(CLEAN_INPUT)

print("=" * 60)
print("‚úÖ SUCCESS!")
print("=" * 60)
print(f"\nüìÅ Output Files:")
print(f"   Pose only:     {OUTPUT_POSE}")
print(f"   Seg only:      {OUTPUT_SEG}")
print(f"   Combined:      {OUTPUT_COMBINED}")
print("\nüìã What you get:")
print("   ‚úì RTMLib Wholebody - 133 keypoints, ACCURATE pose")
print("   ‚úì Clean face mesh with proper jawline")
print("   ‚úì Reliable hand tracking")
print("   ‚úì Sapiens segmentation overlay")
print("   ‚úì NO LOWER BODY (knees and ankles hidden)")
print("   ‚úì Same quality as your reference image!")
