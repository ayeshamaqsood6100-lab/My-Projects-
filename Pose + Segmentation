# @title üöÄ RTM + Sapiens Combined Pipeline
# ============================================================================
# RTM (Upper Body Pose) + Sapiens (Segmentation)
# ============================================================================
# RTMW-x for pose estimation (upper body only)
# Sapiens 1B for segmentation
# Side-by-side output
# ============================================================================

import os
import sys
import subprocess
import torch
import cv2
import numpy as np
import gc
from tqdm import tqdm
from huggingface_hub import hf_hub_download

# ============================================================================
# PHASE 0: INSTALL DEPENDENCIES
# ============================================================================
print("üîß PHASE 0: INSTALLING DEPENDENCIES")
def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

install("rtmlib")
install("onnxruntime-gpu")
install("torch")
install("torchvision")
install("opencv-python")
install("tqdm")
install("huggingface_hub")
print("‚úÖ Dependencies installed!\n")

# ============================================================================
# PHASE 1: SETUP & MOUNT DRIVE
# ============================================================================
print("üöÄ PHASE 1: SETUP")
try:
    from google.colab import drive
    if not os.path.exists('/content/drive'):
        drive.mount('/content/drive')
    IN_COLAB = True
    print("‚úÖ Google Drive mounted!")
except ImportError:
    IN_COLAB = False
    print("‚ö†Ô∏è Not running in Colab.")

# ============================================================================
# PATHS CONFIGURATION
# ============================================================================
VIDEO_FILENAME = "DTQA.mp4"
BASE_PATH = "/content/drive/MyDrive" if IN_COLAB else "."
VIDEO_FOLDER = "ConnectHearTestVideos"
RAW_INPUT = os.path.join(BASE_PATH, VIDEO_FOLDER, VIDEO_FILENAME)
CLEAN_INPUT = "/content/cleaned_input.mp4" if IN_COLAB else "./cleaned_input.mp4"

OUTPUT_POSE_AVI = "/content/temp_pose.avi"
OUTPUT_SEG_AVI = "/content/temp_seg.avi"
OUTPUT_COMBINED_AVI = "/content/temp_combined.avi"
OUTPUT_FINAL = os.path.join(BASE_PATH, f"RTM_SAPIENS_{VIDEO_FILENAME}")

# ============================================================================
# DOWNLOAD SAPIENS SEGMENTATION MODEL
# ============================================================================
print("\nüì¶ Downloading Sapiens Segmentation Model (1B)...")
MODEL_DIR = "/content/sapiens_models" if IN_COLAB else "./sapiens_models"
SEG_MODEL_PATH = hf_hub_download(
    repo_id="facebook/sapiens-seg-1b-torchscript",
    filename="sapiens_1b_goliath_best_goliath_mIoU_7994_epoch_151_torchscript.pt2",
    local_dir=MODEL_DIR
)
print("‚úÖ Sapiens Model Ready!\n")

# ============================================================================
# VIDEO REPAIR & VALIDATION
# ============================================================================
print(f"üîç Checking Input: {RAW_INPUT}")
if not os.path.exists(RAW_INPUT):
    print(f"‚ùå ERROR: File not found at {RAW_INPUT}")
    print("   Please check the filename in your Google Drive.")
    sys.exit(1)

print("üîß Repairing video with FFmpeg...")
subprocess.run(
    f'ffmpeg -y -loglevel error -i "{RAW_INPUT}" -c:v libx264 -preset fast -crf 23 -pix_fmt yuv420p "{CLEAN_INPUT}"',
    shell=True
)

if not os.path.exists(CLEAN_INPUT):
    print("‚ùå Critical Error: FFmpeg failed to process video.")
    sys.exit(1)
else:
    print("‚úÖ Video Repaired and Ready.\n")

# ============================================================================
# SAPIENS SEGMENTATION SETUP
# ============================================================================
BODY_PART_COLORS = np.array([
    [0,0,0], [255,220,200], [255,200,170], [100,200,100], [50,255,50], [255,165,0],
    [0,255,128], [255,140,0], [0,255,255], [255,100,100], [100,100,255], [255,100,255],
    [50,50,200], [200,50,200], [0,200,200], [200,200,0], [139,90,43], [255,210,180],
    [255,255,255], [255,100,150], [255,150,150], [255,120,120], [160,110,60], [160,110,60],
    [100,160,220], [100,160,220], [255,200,160], [200,160,110]
], dtype=np.uint8)

def preprocess_sapiens(img):
    """Preprocess frame for Sapiens model"""
    img = cv2.resize(img, (768, 1024))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
    img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]
    return torch.from_numpy(img.transpose(2, 0, 1)).unsqueeze(0).float()

def draw_segmentation(seg_map, w, h, original_frame, blend_alpha=0.7):
    """Draw segmentation overlay"""
    seg_colored = BODY_PART_COLORS[seg_map]
    seg_colored = cv2.resize(seg_colored, (w, h), interpolation=cv2.INTER_LINEAR)
    mask = (seg_map != 0).astype(np.uint8)
    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_LINEAR)
    mask = np.expand_dims(mask, axis=2)
    blended = (seg_colored * blend_alpha + original_frame * (1 - blend_alpha)).astype(np.uint8)
    return np.where(mask > 0, blended, original_frame).astype(np.uint8)

# ============================================================================
# VIDEO METADATA
# ============================================================================
print("üé¨ Reading Video Metadata...")
cap = cv2.VideoCapture(CLEAN_INPUT)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
fps = cap.get(cv2.CAP_PROP_FPS) or 30
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
width = (width // 2) * 2
height = (height // 2) * 2
cap.release()

print(f"   Video Info: {width}x{height}, {total_frames} frames, {fps} fps")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"   Device: {device}\n")

# ============================================================================
# PASS 1: RTM POSE ESTIMATION (UPPER BODY ONLY)
# ============================================================================
print("üèÉ PASS 1/3: RTM POSE ESTIMATION (Upper Body Only)")

from rtmlib import Wholebody, draw_skeleton

rtm_model = Wholebody(to_openpose=False, mode='performance', backend='onnxruntime', device='cuda')

cap = cv2.VideoCapture(CLEAN_INPUT)
fourcc = cv2.VideoWriter_fourcc(*'MJPG')
out_pose = cv2.VideoWriter(OUTPUT_POSE_AVI, fourcc, fps, (width, height))

processed_count = 0
with tqdm(total=total_frames, desc="RTM Pose") as pbar:
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame = cv2.resize(frame, (width, height))
        
        # RTM Inference
        keypoints, scores = rtm_model(frame)
        
        # Single person filter + Remove lower body
        if len(keypoints) > 0:
            avg_scores = [np.mean(s) for s in scores]
            best_idx = np.argmax(avg_scores)
            keypoints = keypoints[best_idx:best_idx+1]
            scores = scores[best_idx:best_idx+1]
            
            # Remove legs (indices 13-16: knees and ankles)
            scores[:, 13:17] = 0.0
        
        # Draw skeleton with thicker lines
        result_frame = draw_skeleton(frame.copy(), keypoints, scores, 
                                     kpt_thr=0.5, line_width=2, radius=2)
        
        out_pose.write(result_frame)
        processed_count += 1
        pbar.update(1)

cap.release()
out_pose.release()
del rtm_model
gc.collect()
torch.cuda.empty_cache()

print(f"‚úÖ RTM Pose Done. Processed {processed_count} frames.\n")
if processed_count == 0:
    print("‚ùå ERROR: No frames processed!")
    sys.exit(1)

# ============================================================================
# PASS 2: SAPIENS SEGMENTATION
# ============================================================================
print("üèÉ PASS 2/3: SAPIENS SEGMENTATION")

seg_model = torch.jit.load(SEG_MODEL_PATH, map_location=device).eval()

cap = cv2.VideoCapture(CLEAN_INPUT)
out_seg = cv2.VideoWriter(OUTPUT_SEG_AVI, fourcc, fps, (width, height))

with tqdm(total=total_frames, desc="Segmentation") as pbar:
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame = cv2.resize(frame, (width, height))
        
        # Sapiens Inference
        tensor = preprocess_sapiens(frame).to(device)
        with torch.no_grad():
            seg_out = seg_model(tensor)
        
        seg_logits = (seg_out[0] if isinstance(seg_out, tuple) else seg_out)[0].cpu().numpy()
        seg_map = np.argmax(seg_logits, axis=0)
        
        result_frame = draw_segmentation(seg_map, width, height, frame.copy())
        out_seg.write(result_frame)
        pbar.update(1)

cap.release()
out_seg.release()
del seg_model
gc.collect()
torch.cuda.empty_cache()

print("‚úÖ Sapiens Segmentation Done.\n")

# ============================================================================
# PASS 3: STITCHING SIDE-BY-SIDE
# ============================================================================
print("üßµ PASS 3/3: STITCHING VIDEOS...")

cap1 = cv2.VideoCapture(OUTPUT_POSE_AVI)
cap2 = cv2.VideoCapture(OUTPUT_SEG_AVI)
out_final_avi = cv2.VideoWriter(OUTPUT_COMBINED_AVI, fourcc, fps, (width * 2, height))

with tqdm(total=total_frames, desc="Stitching") as pbar:
    while True:
        r1, f1 = cap1.read()
        r2, f2 = cap2.read()
        if not r1 or not r2:
            break
        out_final_avi.write(np.hstack([f1, f2]))
        pbar.update(1)

cap1.release()
cap2.release()
out_final_avi.release()

# ============================================================================
# FINAL CONVERSION TO MP4
# ============================================================================
print("\nüîÑ Converting to MP4...")
subprocess.run(
    f'ffmpeg -y -loglevel error -i "{OUTPUT_COMBINED_AVI}" -c:v libx264 -pix_fmt yuv420p "{OUTPUT_FINAL}"',
    shell=True
)

# Cleanup temp files
for temp_file in [OUTPUT_POSE_AVI, OUTPUT_SEG_AVI, OUTPUT_COMBINED_AVI, CLEAN_INPUT]:
    if os.path.exists(temp_file):
        os.remove(temp_file)

print("=" * 60)
print(f"‚úÖ SUCCESS! Video Saved: {OUTPUT_FINAL}")
print("=" * 60)
